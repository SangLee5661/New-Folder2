{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQnBjbzt6NfDVwg2rBCXAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangLee5661/New-Folder2/blob/main/%EC%8B%A0%EC%9A%A9%EC%9C%84%ED%97%98%EB%8F%84%EB%B6%84%EC%84%9D_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmoRU0vhqTxI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 마운트 및 데이터 로드\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A6VkZBpqXQ_",
        "outputId": "1ae13017-86b4-438c-fa7f-35d30381d951"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드\n",
        "train_path = '/content/drive/MyDrive/card_train.csv'\n",
        "test_path = '/content/drive/MyDrive/card_test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(\"데이터 로드 완료:\")\n",
        "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
        "print(\"\\nTrain Segment 분포:\")\n",
        "print(train_df['Segment'].value_counts(dropna=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9EFtZr7qXYX",
        "outputId": "8135df75-6af5-4760-9ad5-20ee5ae162ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드 완료:\n",
            "Train: (70560, 738), Test: (1440, 737)\n",
            "\n",
            "Train Segment 분포:\n",
            "Segment\n",
            "E    56505\n",
            "D    10270\n",
            "C     3753\n",
            "A       28\n",
            "B        4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유효 레이블 필터링 (train 데이터만)\n",
        "valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "train_df = train_df[train_df['Segment'].isin(valid_labels)]\n",
        "print(f\"\\n유효 레이블 필터링 후 Train: {train_df.shape}\")\n",
        "print(\"필터링 후 Segment 분포:\")\n",
        "print(train_df['Segment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ0cazUgqXbi",
        "outputId": "e8c920a3-77ac-434e-aa22-2d2b2d989b74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "유효 레이블 필터링 후 Train: (70560, 738)\n",
            "필터링 후 Segment 분포:\n",
            "Segment\n",
            "E    56505\n",
            "D    10270\n",
            "C     3753\n",
            "A       28\n",
            "B        4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 정의\n",
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"데이터 전처리 통합 함수\"\"\"\n",
        "    # 불필요한 컬럼 제거\n",
        "    drop_cols = ['ID', 'Unnamed: 0.1', 'Segment.1']\n",
        "    if is_train:\n",
        "        X = df.drop(columns=drop_cols + ['Segment'], errors='ignore')\n",
        "        y = df['Segment'] if 'Segment' in df.columns else None\n",
        "    else:\n",
        "        X = df.drop(columns=drop_cols, errors='ignore')\n",
        "        y = None\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "qvAeHG-_qXfE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "X_full, y_full = preprocess_data(train_df, is_train=True)\n",
        "X_final_test, _ = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "print(f\"\\n전처리 후 형태:\")\n",
        "print(f\"X_full: {X_full.shape}, y_full: {len(y_full)}\")\n",
        "print(f\"X_final_test: {X_final_test.shape}\")\n",
        "\n",
        "# Train 데이터를 8:2로 분할 (내부 검증용)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "print(f\"\\n8:2 분할 결과:\")\n",
        "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "print(\"\\nTrain set 등급 분포:\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())\n",
        "print(\"Validation set 등급 분포:\")\n",
        "print(pd.Series(y_val).value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNOnZnoMqXiL",
        "outputId": "b14de6d5-5319-47eb-dc67-bf9c66f673af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "전처리 후 형태:\n",
            "X_full: (70560, 734), y_full: 70560\n",
            "X_final_test: (1440, 735)\n",
            "\n",
            "8:2 분할 결과:\n",
            "Train: (56448, 734), Validation: (14112, 734)\n",
            "\n",
            "Train set 등급 분포:\n",
            "Segment\n",
            "A       22\n",
            "B        3\n",
            "C     3003\n",
            "D     8216\n",
            "E    45204\n",
            "Name: count, dtype: int64\n",
            "Validation set 등급 분포:\n",
            "Segment\n",
            "A        6\n",
            "B        1\n",
            "C      750\n",
            "D     2054\n",
            "E    11301\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처 엔지니어링 및 전처리 클래스\n",
        "class FeatureProcessor:\n",
        "    def __init__(self):\n",
        "        self.encoders = {}\n",
        "        self.fill_values = {}\n",
        "        self.feature_names = None\n",
        "\n",
        "    def fit_transform(self, X_train):\n",
        "        \"\"\"학습 데이터로 fit하고 transform\"\"\"\n",
        "        X_processed = X_train.copy()\n",
        "\n",
        "        # 범주형 변수 인코딩\n",
        "        categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
        "            self.encoders[col] = le\n",
        "\n",
        "        # 결측치 처리를 위한 중앙값 저장\n",
        "        numeric_cols = X_processed.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            self.fill_values[col] = X_processed[col].median()\n",
        "\n",
        "        # 결측치 및 무한값 처리\n",
        "        X_processed = self._handle_missing_values(X_processed)\n",
        "\n",
        "        # 피처 엔지니어링\n",
        "        X_processed = self._create_features(X_processed)\n",
        "\n",
        "        self.feature_names = X_processed.columns.tolist()\n",
        "        return X_processed\n",
        "\n",
        "    def transform(self, X_test):\n",
        "        \"\"\"학습된 변환을 테스트 데이터에 적용\"\"\"\n",
        "        X_processed = X_test.copy()\n",
        "\n",
        "        # 범주형 변수 인코딩 (fit된 encoder 사용)\n",
        "        for col, encoder in self.encoders.items():\n",
        "            if col in X_processed.columns:\n",
        "                # 새로운 범주 처리\n",
        "                X_processed[col] = X_processed[col].astype(str)\n",
        "                unknown_mask = ~X_processed[col].isin(encoder.classes_)\n",
        "                if unknown_mask.any():\n",
        "                    # 가장 빈번한 클래스로 대체\n",
        "                    most_frequent = encoder.classes_[0]\n",
        "                    X_processed.loc[unknown_mask, col] = most_frequent\n",
        "                X_processed[col] = encoder.transform(X_processed[col])\n",
        "\n",
        "        # 결측치 처리\n",
        "        X_processed = self._handle_missing_values(X_processed)\n",
        "\n",
        "        # 피처 엔지니어링\n",
        "        X_processed = self._create_features(X_processed)\n",
        "\n",
        "        # 학습 시와 동일한 컬럼만 유지\n",
        "        missing_cols = set(self.feature_names) - set(X_processed.columns)\n",
        "        extra_cols = set(X_processed.columns) - set(self.feature_names)\n",
        "\n",
        "        # 누락된 컬럼은 0으로 채움\n",
        "        for col in missing_cols:\n",
        "            X_processed[col] = 0\n",
        "\n",
        "        # 추가 컬럼 제거\n",
        "        X_processed = X_processed.drop(columns=list(extra_cols), errors='ignore')\n",
        "\n",
        "        # 컬럼 순서 맞춤\n",
        "        X_processed = X_processed[self.feature_names]\n",
        "\n",
        "        return X_processed\n",
        "\n",
        "    def _handle_missing_values(self, X):\n",
        "        \"\"\"결측치 및 무한값 처리\"\"\"\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        for col in X.columns:\n",
        "            if col in self.fill_values:\n",
        "                X[col] = X[col].fillna(self.fill_values[col])\n",
        "            else:\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 0)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _create_features(self, X):\n",
        "        \"\"\"피처 엔지니어링\"\"\"\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # 사용률 피처 생성\n",
        "        usage_cols = [col for col in X.columns if any(p in col.lower() for p in ['사용금액', '사용액', 'usage', 'amount'])]\n",
        "        limit_cols = [col for col in X.columns if any(p in col.lower() for p in ['한도금액', '한도액', 'limit'])]\n",
        "\n",
        "        created_features = 0\n",
        "        for usage_col in usage_cols[:5]:  # 상위 5개만\n",
        "            for limit_col in limit_cols[:3]:  # 상위 3개만\n",
        "                if any(period in usage_col and period in limit_col for period in ['R3M', 'R6M', 'R12M']):\n",
        "                    ratio_name = f\"사용률_{usage_col.split('_')[-1] if '_' in usage_col else created_features}\"\n",
        "                    safe_limit = X[limit_col].replace(0, 1)\n",
        "                    X_new[ratio_name] = np.clip(X[usage_col] / safe_limit, 0, 5)  # 이상치 제한\n",
        "                    created_features += 1\n",
        "                    if created_features >= 10:  # 최대 10개 피처\n",
        "                        break\n",
        "            if created_features >= 10:\n",
        "                break\n",
        "\n",
        "        # 거래 빈도 피처\n",
        "        count_cols = [col for col in X.columns if any(p in col.lower() for p in ['건수', 'count', '횟수'])]\n",
        "        for i, count_col in enumerate(count_cols[:5]):\n",
        "            if 'R3M' in count_col or '3개월' in count_col:\n",
        "                freq_name = f\"월평균빈도_{i}\"\n",
        "                X_new[freq_name] = X[count_col] / 3\n",
        "\n",
        "        return X_new"
      ],
      "metadata": {
        "id": "pOl22ekpqXli"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개선된 클래스 불균형 처리 함수\n",
        "def handle_class_imbalance(X_train, y_train, strategy='auto'):\n",
        "    \"\"\"클래스 불균형 문제를 다양한 방법으로 해결\"\"\"\n",
        "\n",
        "    class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "    min_count = class_counts.min()\n",
        "    max_count = class_counts.max()\n",
        "\n",
        "    print(\"원본 클래스 분포:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"  {cls}: {count}개 ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n불균형 비율: {max_count/min_count:.1f}:1\")\n",
        "\n",
        "    # 전략 1: 극소수 클래스 제거 후 SMOTE\n",
        "    if strategy == 'auto' or strategy == 'remove_minority':\n",
        "        # 5개 미만인 클래스는 제거\n",
        "        valid_classes = class_counts[class_counts >= 5].index\n",
        "\n",
        "        if len(valid_classes) < len(class_counts):\n",
        "            print(f\"극소수 클래스 제거: {set(class_counts.index) - set(valid_classes)}\")\n",
        "            mask = pd.Series(y_train).isin(valid_classes)\n",
        "            X_train_filtered = X_train[mask]\n",
        "            y_train_filtered = pd.Series(y_train)[mask].values\n",
        "\n",
        "            # 필터링 후 SMOTE 시도\n",
        "            return apply_smote_or_weights(X_train_filtered, y_train_filtered)\n",
        "\n",
        "    # 전략 2: 클래스 가중치만 사용\n",
        "    if strategy == 'auto' or strategy == 'weights_only':\n",
        "        print(\"SMOTE 대신 클래스 가중치 사용\")\n",
        "        return X_train, y_train, True  # use_class_weight=True\n",
        "\n",
        "    # 전략 3: 수동 오버샘플링\n",
        "    if strategy == 'manual_oversample':\n",
        "        return manual_oversample(X_train, y_train)\n",
        "\n",
        "    return X_train, y_train, True\n",
        "\n",
        "def apply_smote_or_weights(X_train, y_train):\n",
        "    \"\"\"SMOTE 적용 또는 클래스 가중치 사용\"\"\"\n",
        "    class_counts = pd.Series(y_train).value_counts()\n",
        "    min_count = class_counts.min()\n",
        "\n",
        "    if min_count >= 5:\n",
        "        try:\n",
        "            k_neighbors = min(3, min_count - 1)  # k_neighbors 줄임\n",
        "            smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "            X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(f\"✅ SMOTE 적용 완료 (k_neighbors={k_neighbors})\")\n",
        "            print(\"SMOTE 적용 후 클래스 분포:\")\n",
        "            balanced_counts = pd.Series(y_balanced).value_counts().sort_index()\n",
        "            for cls, count in balanced_counts.items():\n",
        "                print(f\"  {cls}: {count}개\")\n",
        "\n",
        "            return X_balanced, y_balanced, False  # use_class_weight=False\n",
        "        except Exception as e:\n",
        "            print(f\"❌ SMOTE 실패: {str(e)}\")\n",
        "\n",
        "    print(\"SMOTE 적용 불가 → 클래스 가중치 사용\")\n",
        "    return X_train, y_train, True  # use_class_weight=True\n",
        "\n",
        "def manual_oversample(X_train, y_train, target_ratio=0.3):\n",
        "    \"\"\"수동 오버샘플링 (복제 방식)\"\"\"\n",
        "    class_counts = pd.Series(y_train).value_counts()\n",
        "    max_count = class_counts.max()\n",
        "    target_min_count = max(int(max_count * target_ratio), 10)\n",
        "\n",
        "    X_resampled = []\n",
        "    y_resampled = []\n",
        "\n",
        "    for cls in class_counts.index:\n",
        "        cls_mask = pd.Series(y_train) == cls\n",
        "        cls_X = X_train[cls_mask]\n",
        "        cls_y = y_train[cls_mask]\n",
        "\n",
        "        current_count = len(cls_y)\n",
        "        if current_count < target_min_count:\n",
        "            # 복제로 샘플 증가\n",
        "            n_copies = target_min_count // current_count\n",
        "            n_extra = target_min_count % current_count\n",
        "\n",
        "            # 기본 복제\n",
        "            for _ in range(n_copies):\n",
        "                X_resampled.append(cls_X)\n",
        "                y_resampled.extend(cls_y)\n",
        "\n",
        "            # 추가 샘플 (랜덤 선택)\n",
        "            if n_extra > 0:\n",
        "                extra_idx = np.random.choice(len(cls_X), n_extra, replace=False)\n",
        "                X_resampled.append(cls_X.iloc[extra_idx])\n",
        "                y_resampled.extend(cls_y[extra_idx])\n",
        "        else:\n",
        "            X_resampled.append(cls_X)\n",
        "            y_resampled.extend(cls_y)\n",
        "\n",
        "    X_final = pd.concat(X_resampled, ignore_index=True)\n",
        "    y_final = np.array(y_resampled)\n",
        "\n",
        "    print(\"수동 오버샘플링 완료:\")\n",
        "    final_counts = pd.Series(y_final).value_counts().sort_index()\n",
        "    for cls, count in final_counts.items():\n",
        "        print(f\"  {cls}: {count}개\")\n",
        "\n",
        "    return X_final, y_final, False"
      ],
      "metadata": {
        "id": "mhACZYSaqXn4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처 처리 및 모델 학습\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"=== 피처 처리 및 모델 학습 ===\")\n",
        "\n",
        "# 피처 프로세서 학습\n",
        "processor = FeatureProcessor()\n",
        "X_train_processed = processor.fit_transform(X_train)\n",
        "X_val_processed = processor.transform(X_val)\n",
        "\n",
        "print(f\"피처 처리 완료:\")\n",
        "print(f\"처리된 Train: {X_train_processed.shape}\")\n",
        "print(f\"처리된 Validation: {X_val_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUaOgX4Zq2nv",
        "outputId": "495435cb-5003-4c32-c42d-fe1d8518a1b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "=== 피처 처리 및 모델 학습 ===\n",
            "피처 처리 완료:\n",
            "처리된 Train: (56448, 736)\n",
            "처리된 Validation: (14112, 736)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 불균형 처리 (학습 데이터에만!)\n",
        "print(\"클래스 불균형 처리 시작...\")\n",
        "X_train_balanced, y_train_balanced, use_class_weight = handle_class_imbalance(\n",
        "    X_train_processed, y_train, strategy='auto'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-8gFvSoq2si",
        "outputId": "7e4e25a7-4af6-4e1b-bf00-289adc3f84aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스 불균형 처리 시작...\n",
            "원본 클래스 분포:\n",
            "  A: 22개 (0.0%)\n",
            "  B: 3개 (0.0%)\n",
            "  C: 3003개 (5.3%)\n",
            "  D: 8216개 (14.6%)\n",
            "  E: 45204개 (80.1%)\n",
            "\n",
            "불균형 비율: 15068.0:1\n",
            "극소수 클래스 제거: {'B'}\n",
            "✅ SMOTE 적용 완료 (k_neighbors=3)\n",
            "SMOTE 적용 후 클래스 분포:\n",
            "  A: 45204개\n",
            "  C: 45204개\n",
            "  D: 45204개\n",
            "  E: 45204개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 비교 (클래스 가중치 고려)\n",
        "models = {}\n",
        "if use_class_weight:\n",
        "    print(\"클래스 가중치 적용 모델 사용\")\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "    }\n",
        "else:\n",
        "    print(\"일반 모델 사용 (데이터 균형 처리됨)\")\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "print(f\"\\n=== 모델 성능 비교 ===\")\n",
        "best_model = None\n",
        "best_score = 0\n",
        "best_name = \"\"\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        # 학습\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # 검증\n",
        "        y_val_pred = model.predict(X_val_processed)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "        # 교차 검증\n",
        "        cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, cv=3, scoring='accuracy')\n",
        "        cv_mean = cv_scores.mean()\n",
        "\n",
        "        print(f\"{name:20s}: 검증 정확도 {accuracy:.3f}, CV 평균 {cv_mean:.3f}\")\n",
        "\n",
        "        model_results[name] = {\n",
        "            'model': model,\n",
        "            'val_accuracy': accuracy,\n",
        "            'cv_accuracy': cv_mean\n",
        "        }\n",
        "\n",
        "        if accuracy > best_score:\n",
        "            best_score = accuracy\n",
        "            best_model = model\n",
        "            best_name = name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{name} 학습 실패: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Tt0pnEq2xJ",
        "outputId": "0bff8590-40b4-4442-9bfc-598b94519374"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일반 모델 사용 (데이터 균형 처리됨)\n",
            "\n",
            "=== 모델 성능 비교 ===\n",
            "RandomForest        : 검증 정확도 0.870, CV 평균 0.955\n",
            "GradientBoosting    : 검증 정확도 0.868, CV 평균 0.882\n",
            "LogisticRegression  : 검증 정확도 0.761, CV 평균 0.766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적 모델 선택 및 상세 평가\n",
        "if best_model is not None:\n",
        "    print(f\"\\n🏆 최적 모델: {best_name} (검증 정확도: {best_score:.3f})\")\n",
        "\n",
        "    # 상세 분류 보고서\n",
        "    y_val_pred = best_model.predict(X_val_processed)\n",
        "    print(f\"\\n=== 상세 분류 성능 ===\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "    # 등급별 정확도\n",
        "    print(\"\\n등급별 예측 정확도:\")\n",
        "    for grade in sorted(set(y_val)):\n",
        "        grade_mask = np.array(y_val) == grade\n",
        "        if grade_mask.sum() > 0:\n",
        "            grade_accuracy = accuracy_score(\n",
        "                np.array(y_val)[grade_mask],\n",
        "                y_val_pred[grade_mask]\n",
        "            )\n",
        "            print(f\"{grade}등급: {grade_accuracy:.3f} ({grade_mask.sum()}개 샘플)\")\n",
        "\n",
        "    # 피처 중요도 (RandomForest인 경우)\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        print(f\"\\n=== 상위 20개 중요 피처 ===\")\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_train_processed.columns,\n",
        "            'importance': best_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        for i, (_, row) in enumerate(feature_importance.head(20).iterrows()):\n",
        "            print(f\"{i+1:2d}. {row['feature']:35s} {row['importance']:.4f}\")\n",
        "\n",
        "    # 최종 테스트 데이터 예측 준비\n",
        "    print(f\"\\n=== 최종 예측 준비 ===\")\n",
        "    X_final_test_processed = processor.transform(X_final_test)\n",
        "    print(f\"최종 테스트 데이터 처리 완료: {X_final_test_processed.shape}\")\n",
        "\n",
        "    # 최종 예측\n",
        "    final_predictions = best_model.predict(X_final_test_processed)\n",
        "    prediction_proba = best_model.predict_proba(X_final_test_processed)\n",
        "\n",
        "    print(\"최종 예측 등급 분포:\")\n",
        "    print(pd.Series(final_predictions).value_counts().sort_index())\n",
        "\n",
        "    # 예측 결과 저장을 위한 DataFrame 생성\n",
        "    results_df = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'Predicted_Segment': final_predictions\n",
        "    })\n",
        "\n",
        "    # 예측 확률도 추가 (각 등급별)\n",
        "    prob_cols = [f'Prob_{cls}' for cls in best_model.classes_]\n",
        "    for i, col in enumerate(prob_cols):\n",
        "        results_df[col] = prediction_proba[:, i]\n",
        "\n",
        "    print(f\"\\n예측 결과 샘플:\")\n",
        "    print(results_df.head())\n",
        "\n",
        "    # 결과 저장 (선택사항)\n",
        "    # results_df.to_csv('/content/drive/MyDrive/card_predictions.csv', index=False)\n",
        "    # print(\"예측 결과가 저장되었습니다.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 모든 모델 학습에 실패했습니다.\")\n",
        "\n",
        "# 학습 검증 요약\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"=== 학습 검증 요약 ===\")\n",
        "print(f\"✅ 데이터 분할: Train 80% ({len(y_train)}개), Validation 20% ({len(y_val)}개)\")\n",
        "if use_class_weight:\n",
        "    print(f\"✅ 클래스 불균형 처리: 가중치 적용 (극소수 클래스 처리)\")\n",
        "else:\n",
        "    print(f\"✅ 클래스 불균형 처리: 데이터 증강 완료 ({len(y_train_balanced)}개)\")\n",
        "print(f\"✅ 피처 처리: {X_train_processed.shape[1]}개 피처 (파생 피처 포함)\")\n",
        "if best_model:\n",
        "    print(f\"✅ 모델 선택: {best_name} (검증 정확도 {best_score:.3f})\")\n",
        "    print(f\"✅ 최종 예측: {len(final_predictions)}개 테스트 샘플 예측 완료\")\n",
        "\n",
        "print(\"\\n클래스별 처리 결과:\")\n",
        "final_class_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
        "for cls, count in final_class_counts.items():\n",
        "    original_count = pd.Series(y_train).value_counts().get(cls, 0)\n",
        "    print(f\"  {cls}: {original_count} → {count} ({'증강' if count > original_count else '유지'})\")\n",
        "\n",
        "print(\"\\n권장사항:\")\n",
        "if use_class_weight:\n",
        "    print(\"1. 극소수 클래스가 제거되었으므로 전체 클래스 예측 성능 확인 필요\")\n",
        "    print(\"2. 더 많은 데이터 수집으로 클래스 불균형 근본 해결 권장\")\n",
        "else:\n",
        "    print(\"1. 데이터 증강이 완료되어 균형잡힌 학습 가능\")\n",
        "print(\"3. 하이퍼파라미터 튜닝으로 성능 개선 가능\")\n",
        "print(\"4. 앙상블 모델 적용 고려\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6s4YEfBq21s",
        "outputId": "61408d8a-bd15-4e12-be51-653670bf3eaf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🏆 최적 모델: RandomForest (검증 정확도: 0.870)\n",
            "\n",
            "=== 상세 분류 성능 ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.00      0.00      0.00         6\n",
            "           B       0.00      0.00      0.00         1\n",
            "           C       0.65      0.60      0.62       750\n",
            "           D       0.58      0.66      0.62      2054\n",
            "           E       0.95      0.93      0.94     11301\n",
            "\n",
            "    accuracy                           0.87     14112\n",
            "   macro avg       0.44      0.44      0.44     14112\n",
            "weighted avg       0.88      0.87      0.87     14112\n",
            "\n",
            "\n",
            "등급별 예측 정확도:\n",
            "A등급: 0.000 (6개 샘플)\n",
            "B등급: 0.000 (1개 샘플)\n",
            "C등급: 0.596 (750개 샘플)\n",
            "D등급: 0.661 (2054개 샘플)\n",
            "E등급: 0.927 (11301개 샘플)\n",
            "\n",
            "=== 상위 20개 중요 피처 ===\n",
            " 1. 정상청구원금_B0M                          0.0383\n",
            " 2. 청구금액_R6M                            0.0328\n",
            " 3. 정상청구원금_B5M                          0.0279\n",
            " 4. 청구금액_R3M                            0.0267\n",
            " 5. 카드이용한도금액                            0.0221\n",
            " 6. 카드이용한도금액_B2M                        0.0204\n",
            " 7. 정상청구원금_B2M                          0.0200\n",
            " 8. 카드이용한도금액_B1M                        0.0182\n",
            " 9. 이용금액_오프라인_B0M                       0.0165\n",
            "10. 정상입금원금_B2M                          0.0141\n",
            "11. 평잔_6M                               0.0140\n",
            "12. 청구금액_B0                             0.0140\n",
            "13. 정상입금원금_B5M                          0.0125\n",
            "14. 이용금액대                               0.0120\n",
            "15. CA한도금액                              0.0120\n",
            "16. 정상입금원금_B0M                          0.0118\n",
            "17. 이용금액_R3M_신용체크                       0.0113\n",
            "18. 이용금액_일시불_R12M                       0.0111\n",
            "19. _1순위카드이용금액                          0.0107\n",
            "20. 입회일자_신용                             0.0105\n",
            "\n",
            "=== 최종 예측 준비 ===\n",
            "최종 테스트 데이터 처리 완료: (1440, 736)\n",
            "최종 예측 등급 분포:\n",
            "C      73\n",
            "D     227\n",
            "E    1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "예측 결과 샘플:\n",
            "             ID Predicted_Segment  Prob_A  Prob_C  Prob_D  Prob_E\n",
            "0  TRAIN_291453                 D     0.0    0.12    0.57    0.31\n",
            "1  TRAIN_104115                 E     0.0    0.29    0.29    0.42\n",
            "2  TRAIN_129210                 E     0.0    0.05    0.27    0.68\n",
            "3  TRAIN_194246                 E     0.0    0.01    0.05    0.94\n",
            "4  TRAIN_183742                 E     0.0    0.01    0.06    0.93\n",
            "\n",
            "==================================================\n",
            "=== 학습 검증 요약 ===\n",
            "✅ 데이터 분할: Train 80% (56448개), Validation 20% (14112개)\n",
            "✅ 클래스 불균형 처리: 데이터 증강 완료 (180816개)\n",
            "✅ 피처 처리: 736개 피처 (파생 피처 포함)\n",
            "✅ 모델 선택: RandomForest (검증 정확도 0.870)\n",
            "✅ 최종 예측: 1440개 테스트 샘플 예측 완료\n",
            "\n",
            "클래스별 처리 결과:\n",
            "  A: 22 → 45204 (증강)\n",
            "  C: 3003 → 45204 (증강)\n",
            "  D: 8216 → 45204 (증강)\n",
            "  E: 45204 → 45204 (유지)\n",
            "\n",
            "권장사항:\n",
            "1. 데이터 증강이 완료되어 균형잡힌 학습 가능\n",
            "3. 하이퍼파라미터 튜닝으로 성능 개선 가능\n",
            "4. 앙상블 모델 적용 고려\n"
          ]
        }
      ]
    }
  ]
}