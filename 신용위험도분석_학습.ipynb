{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQnBjbzt6NfDVwg2rBCXAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangLee5661/New-Folder2/blob/main/%EC%8B%A0%EC%9A%A9%EC%9C%84%ED%97%98%EB%8F%84%EB%B6%84%EC%84%9D_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmoRU0vhqTxI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„° ë¡œë“œ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A6VkZBpqXQ_",
        "outputId": "1ae13017-86b4-438c-fa7f-35d30381d951"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "train_path = '/content/drive/MyDrive/card_train.csv'\n",
        "test_path = '/content/drive/MyDrive/card_test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "print(\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
        "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
        "print(\"\\nTrain Segment ë¶„í¬:\")\n",
        "print(train_df['Segment'].value_counts(dropna=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9EFtZr7qXYX",
        "outputId": "8135df75-6af5-4760-9ad5-20ee5ae162ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\n",
            "Train: (70560, 738), Test: (1440, 737)\n",
            "\n",
            "Train Segment ë¶„í¬:\n",
            "Segment\n",
            "E    56505\n",
            "D    10270\n",
            "C     3753\n",
            "A       28\n",
            "B        4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìœ íš¨ ë ˆì´ë¸” í•„í„°ë§ (train ë°ì´í„°ë§Œ)\n",
        "valid_labels = ['A', 'B', 'C', 'D', 'E']\n",
        "train_df = train_df[train_df['Segment'].isin(valid_labels)]\n",
        "print(f\"\\nìœ íš¨ ë ˆì´ë¸” í•„í„°ë§ í›„ Train: {train_df.shape}\")\n",
        "print(\"í•„í„°ë§ í›„ Segment ë¶„í¬:\")\n",
        "print(train_df['Segment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ0cazUgqXbi",
        "outputId": "e8c920a3-77ac-434e-aa22-2d2b2d989b74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ìœ íš¨ ë ˆì´ë¸” í•„í„°ë§ í›„ Train: (70560, 738)\n",
            "í•„í„°ë§ í›„ Segment ë¶„í¬:\n",
            "Segment\n",
            "E    56505\n",
            "D    10270\n",
            "C     3753\n",
            "A       28\n",
            "B        4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def preprocess_data(df, is_train=True):\n",
        "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ í†µí•© í•¨ìˆ˜\"\"\"\n",
        "    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
        "    drop_cols = ['ID', 'Unnamed: 0.1', 'Segment.1']\n",
        "    if is_train:\n",
        "        X = df.drop(columns=drop_cols + ['Segment'], errors='ignore')\n",
        "        y = df['Segment'] if 'Segment' in df.columns else None\n",
        "    else:\n",
        "        X = df.drop(columns=drop_cols, errors='ignore')\n",
        "        y = None\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "qvAeHG-_qXfE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "X_full, y_full = preprocess_data(train_df, is_train=True)\n",
        "X_final_test, _ = preprocess_data(test_df, is_train=False)\n",
        "\n",
        "print(f\"\\nì „ì²˜ë¦¬ í›„ í˜•íƒœ:\")\n",
        "print(f\"X_full: {X_full.shape}, y_full: {len(y_full)}\")\n",
        "print(f\"X_final_test: {X_final_test.shape}\")\n",
        "\n",
        "# Train ë°ì´í„°ë¥¼ 8:2ë¡œ ë¶„í•  (ë‚´ë¶€ ê²€ì¦ìš©)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "print(f\"\\n8:2 ë¶„í•  ê²°ê³¼:\")\n",
        "print(f\"Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "print(\"\\nTrain set ë“±ê¸‰ ë¶„í¬:\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())\n",
        "print(\"Validation set ë“±ê¸‰ ë¶„í¬:\")\n",
        "print(pd.Series(y_val).value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNOnZnoMqXiL",
        "outputId": "b14de6d5-5319-47eb-dc67-bf9c66f673af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ì „ì²˜ë¦¬ í›„ í˜•íƒœ:\n",
            "X_full: (70560, 734), y_full: 70560\n",
            "X_final_test: (1440, 735)\n",
            "\n",
            "8:2 ë¶„í•  ê²°ê³¼:\n",
            "Train: (56448, 734), Validation: (14112, 734)\n",
            "\n",
            "Train set ë“±ê¸‰ ë¶„í¬:\n",
            "Segment\n",
            "A       22\n",
            "B        3\n",
            "C     3003\n",
            "D     8216\n",
            "E    45204\n",
            "Name: count, dtype: int64\n",
            "Validation set ë“±ê¸‰ ë¶„í¬:\n",
            "Segment\n",
            "A        6\n",
            "B        1\n",
            "C      750\n",
            "D     2054\n",
            "E    11301\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\n",
        "class FeatureProcessor:\n",
        "    def __init__(self):\n",
        "        self.encoders = {}\n",
        "        self.fill_values = {}\n",
        "        self.feature_names = None\n",
        "\n",
        "    def fit_transform(self, X_train):\n",
        "        \"\"\"í•™ìŠµ ë°ì´í„°ë¡œ fití•˜ê³  transform\"\"\"\n",
        "        X_processed = X_train.copy()\n",
        "\n",
        "        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
        "        categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
        "            self.encoders[col] = le\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤‘ì•™ê°’ ì €ì¥\n",
        "        numeric_cols = X_processed.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            self.fill_values[col] = X_processed[col].median()\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ë° ë¬´í•œê°’ ì²˜ë¦¬\n",
        "        X_processed = self._handle_missing_values(X_processed)\n",
        "\n",
        "        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
        "        X_processed = self._create_features(X_processed)\n",
        "\n",
        "        self.feature_names = X_processed.columns.tolist()\n",
        "        return X_processed\n",
        "\n",
        "    def transform(self, X_test):\n",
        "        \"\"\"í•™ìŠµëœ ë³€í™˜ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©\"\"\"\n",
        "        X_processed = X_test.copy()\n",
        "\n",
        "        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (fitëœ encoder ì‚¬ìš©)\n",
        "        for col, encoder in self.encoders.items():\n",
        "            if col in X_processed.columns:\n",
        "                # ìƒˆë¡œìš´ ë²”ì£¼ ì²˜ë¦¬\n",
        "                X_processed[col] = X_processed[col].astype(str)\n",
        "                unknown_mask = ~X_processed[col].isin(encoder.classes_)\n",
        "                if unknown_mask.any():\n",
        "                    # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ëŒ€ì²´\n",
        "                    most_frequent = encoder.classes_[0]\n",
        "                    X_processed.loc[unknown_mask, col] = most_frequent\n",
        "                X_processed[col] = encoder.transform(X_processed[col])\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        X_processed = self._handle_missing_values(X_processed)\n",
        "\n",
        "        # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
        "        X_processed = self._create_features(X_processed)\n",
        "\n",
        "        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€\n",
        "        missing_cols = set(self.feature_names) - set(X_processed.columns)\n",
        "        extra_cols = set(X_processed.columns) - set(self.feature_names)\n",
        "\n",
        "        # ëˆ„ë½ëœ ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€\n",
        "        for col in missing_cols:\n",
        "            X_processed[col] = 0\n",
        "\n",
        "        # ì¶”ê°€ ì»¬ëŸ¼ ì œê±°\n",
        "        X_processed = X_processed.drop(columns=list(extra_cols), errors='ignore')\n",
        "\n",
        "        # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶¤\n",
        "        X_processed = X_processed[self.feature_names]\n",
        "\n",
        "        return X_processed\n",
        "\n",
        "    def _handle_missing_values(self, X):\n",
        "        \"\"\"ê²°ì¸¡ì¹˜ ë° ë¬´í•œê°’ ì²˜ë¦¬\"\"\"\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        for col in X.columns:\n",
        "            if col in self.fill_values:\n",
        "                X[col] = X[col].fillna(self.fill_values[col])\n",
        "            else:\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else 0)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _create_features(self, X):\n",
        "        \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\"\"\"\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # ì‚¬ìš©ë¥  í”¼ì²˜ ìƒì„±\n",
        "        usage_cols = [col for col in X.columns if any(p in col.lower() for p in ['ì‚¬ìš©ê¸ˆì•¡', 'ì‚¬ìš©ì•¡', 'usage', 'amount'])]\n",
        "        limit_cols = [col for col in X.columns if any(p in col.lower() for p in ['í•œë„ê¸ˆì•¡', 'í•œë„ì•¡', 'limit'])]\n",
        "\n",
        "        created_features = 0\n",
        "        for usage_col in usage_cols[:5]:  # ìƒìœ„ 5ê°œë§Œ\n",
        "            for limit_col in limit_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ\n",
        "                if any(period in usage_col and period in limit_col for period in ['R3M', 'R6M', 'R12M']):\n",
        "                    ratio_name = f\"ì‚¬ìš©ë¥ _{usage_col.split('_')[-1] if '_' in usage_col else created_features}\"\n",
        "                    safe_limit = X[limit_col].replace(0, 1)\n",
        "                    X_new[ratio_name] = np.clip(X[usage_col] / safe_limit, 0, 5)  # ì´ìƒì¹˜ ì œí•œ\n",
        "                    created_features += 1\n",
        "                    if created_features >= 10:  # ìµœëŒ€ 10ê°œ í”¼ì²˜\n",
        "                        break\n",
        "            if created_features >= 10:\n",
        "                break\n",
        "\n",
        "        # ê±°ë˜ ë¹ˆë„ í”¼ì²˜\n",
        "        count_cols = [col for col in X.columns if any(p in col.lower() for p in ['ê±´ìˆ˜', 'count', 'íšŸìˆ˜'])]\n",
        "        for i, count_col in enumerate(count_cols[:5]):\n",
        "            if 'R3M' in count_col or '3ê°œì›”' in count_col:\n",
        "                freq_name = f\"ì›”í‰ê· ë¹ˆë„_{i}\"\n",
        "                X_new[freq_name] = X[count_col] / 3\n",
        "\n",
        "        return X_new"
      ],
      "metadata": {
        "id": "pOl22ekpqXli"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°œì„ ëœ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def handle_class_imbalance(X_train, y_train, strategy='auto'):\n",
        "    \"\"\"í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í•´ê²°\"\"\"\n",
        "\n",
        "    class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "    min_count = class_counts.min()\n",
        "    max_count = class_counts.max()\n",
        "\n",
        "    print(\"ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"  {cls}: {count}ê°œ ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\në¶ˆê· í˜• ë¹„ìœ¨: {max_count/min_count:.1f}:1\")\n",
        "\n",
        "    # ì „ëµ 1: ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì œê±° í›„ SMOTE\n",
        "    if strategy == 'auto' or strategy == 'remove_minority':\n",
        "        # 5ê°œ ë¯¸ë§Œì¸ í´ë˜ìŠ¤ëŠ” ì œê±°\n",
        "        valid_classes = class_counts[class_counts >= 5].index\n",
        "\n",
        "        if len(valid_classes) < len(class_counts):\n",
        "            print(f\"ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì œê±°: {set(class_counts.index) - set(valid_classes)}\")\n",
        "            mask = pd.Series(y_train).isin(valid_classes)\n",
        "            X_train_filtered = X_train[mask]\n",
        "            y_train_filtered = pd.Series(y_train)[mask].values\n",
        "\n",
        "            # í•„í„°ë§ í›„ SMOTE ì‹œë„\n",
        "            return apply_smote_or_weights(X_train_filtered, y_train_filtered)\n",
        "\n",
        "    # ì „ëµ 2: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš©\n",
        "    if strategy == 'auto' or strategy == 'weights_only':\n",
        "        print(\"SMOTE ëŒ€ì‹  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©\")\n",
        "        return X_train, y_train, True  # use_class_weight=True\n",
        "\n",
        "    # ì „ëµ 3: ìˆ˜ë™ ì˜¤ë²„ìƒ˜í”Œë§\n",
        "    if strategy == 'manual_oversample':\n",
        "        return manual_oversample(X_train, y_train)\n",
        "\n",
        "    return X_train, y_train, True\n",
        "\n",
        "def apply_smote_or_weights(X_train, y_train):\n",
        "    \"\"\"SMOTE ì ìš© ë˜ëŠ” í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©\"\"\"\n",
        "    class_counts = pd.Series(y_train).value_counts()\n",
        "    min_count = class_counts.min()\n",
        "\n",
        "    if min_count >= 5:\n",
        "        try:\n",
        "            k_neighbors = min(3, min_count - 1)  # k_neighbors ì¤„ì„\n",
        "            smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "            X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(f\"âœ… SMOTE ì ìš© ì™„ë£Œ (k_neighbors={k_neighbors})\")\n",
        "            print(\"SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "            balanced_counts = pd.Series(y_balanced).value_counts().sort_index()\n",
        "            for cls, count in balanced_counts.items():\n",
        "                print(f\"  {cls}: {count}ê°œ\")\n",
        "\n",
        "            return X_balanced, y_balanced, False  # use_class_weight=False\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ SMOTE ì‹¤íŒ¨: {str(e)}\")\n",
        "\n",
        "    print(\"SMOTE ì ìš© ë¶ˆê°€ â†’ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©\")\n",
        "    return X_train, y_train, True  # use_class_weight=True\n",
        "\n",
        "def manual_oversample(X_train, y_train, target_ratio=0.3):\n",
        "    \"\"\"ìˆ˜ë™ ì˜¤ë²„ìƒ˜í”Œë§ (ë³µì œ ë°©ì‹)\"\"\"\n",
        "    class_counts = pd.Series(y_train).value_counts()\n",
        "    max_count = class_counts.max()\n",
        "    target_min_count = max(int(max_count * target_ratio), 10)\n",
        "\n",
        "    X_resampled = []\n",
        "    y_resampled = []\n",
        "\n",
        "    for cls in class_counts.index:\n",
        "        cls_mask = pd.Series(y_train) == cls\n",
        "        cls_X = X_train[cls_mask]\n",
        "        cls_y = y_train[cls_mask]\n",
        "\n",
        "        current_count = len(cls_y)\n",
        "        if current_count < target_min_count:\n",
        "            # ë³µì œë¡œ ìƒ˜í”Œ ì¦ê°€\n",
        "            n_copies = target_min_count // current_count\n",
        "            n_extra = target_min_count % current_count\n",
        "\n",
        "            # ê¸°ë³¸ ë³µì œ\n",
        "            for _ in range(n_copies):\n",
        "                X_resampled.append(cls_X)\n",
        "                y_resampled.extend(cls_y)\n",
        "\n",
        "            # ì¶”ê°€ ìƒ˜í”Œ (ëœë¤ ì„ íƒ)\n",
        "            if n_extra > 0:\n",
        "                extra_idx = np.random.choice(len(cls_X), n_extra, replace=False)\n",
        "                X_resampled.append(cls_X.iloc[extra_idx])\n",
        "                y_resampled.extend(cls_y[extra_idx])\n",
        "        else:\n",
        "            X_resampled.append(cls_X)\n",
        "            y_resampled.extend(cls_y)\n",
        "\n",
        "    X_final = pd.concat(X_resampled, ignore_index=True)\n",
        "    y_final = np.array(y_resampled)\n",
        "\n",
        "    print(\"ìˆ˜ë™ ì˜¤ë²„ìƒ˜í”Œë§ ì™„ë£Œ:\")\n",
        "    final_counts = pd.Series(y_final).value_counts().sort_index()\n",
        "    for cls, count in final_counts.items():\n",
        "        print(f\"  {cls}: {count}ê°œ\")\n",
        "\n",
        "    return X_final, y_final, False"
      ],
      "metadata": {
        "id": "mhACZYSaqXn4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í”¼ì²˜ ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"=== í”¼ì²˜ ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ===\")\n",
        "\n",
        "# í”¼ì²˜ í”„ë¡œì„¸ì„œ í•™ìŠµ\n",
        "processor = FeatureProcessor()\n",
        "X_train_processed = processor.fit_transform(X_train)\n",
        "X_val_processed = processor.transform(X_val)\n",
        "\n",
        "print(f\"í”¼ì²˜ ì²˜ë¦¬ ì™„ë£Œ:\")\n",
        "print(f\"ì²˜ë¦¬ëœ Train: {X_train_processed.shape}\")\n",
        "print(f\"ì²˜ë¦¬ëœ Validation: {X_val_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUaOgX4Zq2nv",
        "outputId": "495435cb-5003-4c32-c42d-fe1d8518a1b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "=== í”¼ì²˜ ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ===\n",
            "í”¼ì²˜ ì²˜ë¦¬ ì™„ë£Œ:\n",
            "ì²˜ë¦¬ëœ Train: (56448, 736)\n",
            "ì²˜ë¦¬ëœ Validation: (14112, 736)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (í•™ìŠµ ë°ì´í„°ì—ë§Œ!)\n",
        "print(\"í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì‹œì‘...\")\n",
        "X_train_balanced, y_train_balanced, use_class_weight = handle_class_imbalance(\n",
        "    X_train_processed, y_train, strategy='auto'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-8gFvSoq2si",
        "outputId": "7e4e25a7-4af6-4e1b-bf00-289adc3f84aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì‹œì‘...\n",
            "ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:\n",
            "  A: 22ê°œ (0.0%)\n",
            "  B: 3ê°œ (0.0%)\n",
            "  C: 3003ê°œ (5.3%)\n",
            "  D: 8216ê°œ (14.6%)\n",
            "  E: 45204ê°œ (80.1%)\n",
            "\n",
            "ë¶ˆê· í˜• ë¹„ìœ¨: 15068.0:1\n",
            "ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì œê±°: {'B'}\n",
            "âœ… SMOTE ì ìš© ì™„ë£Œ (k_neighbors=3)\n",
            "SMOTE ì ìš© í›„ í´ë˜ìŠ¤ ë¶„í¬:\n",
            "  A: 45204ê°œ\n",
            "  C: 45204ê°œ\n",
            "  D: 45204ê°œ\n",
            "  E: 45204ê°œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³ ë ¤)\n",
        "models = {}\n",
        "if use_class_weight:\n",
        "    print(\"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš© ëª¨ë¸ ì‚¬ìš©\")\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "    }\n",
        "else:\n",
        "    print(\"ì¼ë°˜ ëª¨ë¸ ì‚¬ìš© (ë°ì´í„° ê· í˜• ì²˜ë¦¬ë¨)\")\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "print(f\"\\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===\")\n",
        "best_model = None\n",
        "best_score = 0\n",
        "best_name = \"\"\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        # í•™ìŠµ\n",
        "        model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # ê²€ì¦\n",
        "        y_val_pred = model.predict(X_val_processed)\n",
        "        accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "        # êµì°¨ ê²€ì¦\n",
        "        cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced, cv=3, scoring='accuracy')\n",
        "        cv_mean = cv_scores.mean()\n",
        "\n",
        "        print(f\"{name:20s}: ê²€ì¦ ì •í™•ë„ {accuracy:.3f}, CV í‰ê·  {cv_mean:.3f}\")\n",
        "\n",
        "        model_results[name] = {\n",
        "            'model': model,\n",
        "            'val_accuracy': accuracy,\n",
        "            'cv_accuracy': cv_mean\n",
        "        }\n",
        "\n",
        "        if accuracy > best_score:\n",
        "            best_score = accuracy\n",
        "            best_model = model\n",
        "            best_name = name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{name} í•™ìŠµ ì‹¤íŒ¨: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Tt0pnEq2xJ",
        "outputId": "0bff8590-40b4-4442-9bfc-598b94519374"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì¼ë°˜ ëª¨ë¸ ì‚¬ìš© (ë°ì´í„° ê· í˜• ì²˜ë¦¬ë¨)\n",
            "\n",
            "=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===\n",
            "RandomForest        : ê²€ì¦ ì •í™•ë„ 0.870, CV í‰ê·  0.955\n",
            "GradientBoosting    : ê²€ì¦ ì •í™•ë„ 0.868, CV í‰ê·  0.882\n",
            "LogisticRegression  : ê²€ì¦ ì •í™•ë„ 0.761, CV í‰ê·  0.766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ìµœì  ëª¨ë¸ ì„ íƒ ë° ìƒì„¸ í‰ê°€\n",
        "if best_model is not None:\n",
        "    print(f\"\\nğŸ† ìµœì  ëª¨ë¸: {best_name} (ê²€ì¦ ì •í™•ë„: {best_score:.3f})\")\n",
        "\n",
        "    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ\n",
        "    y_val_pred = best_model.predict(X_val_processed)\n",
        "    print(f\"\\n=== ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ ===\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "    # ë“±ê¸‰ë³„ ì •í™•ë„\n",
        "    print(\"\\në“±ê¸‰ë³„ ì˜ˆì¸¡ ì •í™•ë„:\")\n",
        "    for grade in sorted(set(y_val)):\n",
        "        grade_mask = np.array(y_val) == grade\n",
        "        if grade_mask.sum() > 0:\n",
        "            grade_accuracy = accuracy_score(\n",
        "                np.array(y_val)[grade_mask],\n",
        "                y_val_pred[grade_mask]\n",
        "            )\n",
        "            print(f\"{grade}ë“±ê¸‰: {grade_accuracy:.3f} ({grade_mask.sum()}ê°œ ìƒ˜í”Œ)\")\n",
        "\n",
        "    # í”¼ì²˜ ì¤‘ìš”ë„ (RandomForestì¸ ê²½ìš°)\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        print(f\"\\n=== ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ ===\")\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_train_processed.columns,\n",
        "            'importance': best_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        for i, (_, row) in enumerate(feature_importance.head(20).iterrows()):\n",
        "            print(f\"{i+1:2d}. {row['feature']:35s} {row['importance']:.4f}\")\n",
        "\n",
        "    # ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤€ë¹„\n",
        "    print(f\"\\n=== ìµœì¢… ì˜ˆì¸¡ ì¤€ë¹„ ===\")\n",
        "    X_final_test_processed = processor.transform(X_final_test)\n",
        "    print(f\"ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {X_final_test_processed.shape}\")\n",
        "\n",
        "    # ìµœì¢… ì˜ˆì¸¡\n",
        "    final_predictions = best_model.predict(X_final_test_processed)\n",
        "    prediction_proba = best_model.predict_proba(X_final_test_processed)\n",
        "\n",
        "    print(\"ìµœì¢… ì˜ˆì¸¡ ë“±ê¸‰ ë¶„í¬:\")\n",
        "    print(pd.Series(final_predictions).value_counts().sort_index())\n",
        "\n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ DataFrame ìƒì„±\n",
        "    results_df = pd.DataFrame({\n",
        "        'ID': test_df['ID'],\n",
        "        'Predicted_Segment': final_predictions\n",
        "    })\n",
        "\n",
        "    # ì˜ˆì¸¡ í™•ë¥ ë„ ì¶”ê°€ (ê° ë“±ê¸‰ë³„)\n",
        "    prob_cols = [f'Prob_{cls}' for cls in best_model.classes_]\n",
        "    for i, col in enumerate(prob_cols):\n",
        "        results_df[col] = prediction_proba[:, i]\n",
        "\n",
        "    print(f\"\\nì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
        "    print(results_df.head())\n",
        "\n",
        "    # ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
        "    # results_df.to_csv('/content/drive/MyDrive/card_predictions.csv', index=False)\n",
        "    # print(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ëª¨ë“  ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# í•™ìŠµ ê²€ì¦ ìš”ì•½\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"=== í•™ìŠµ ê²€ì¦ ìš”ì•½ ===\")\n",
        "print(f\"âœ… ë°ì´í„° ë¶„í• : Train 80% ({len(y_train)}ê°œ), Validation 20% ({len(y_val)}ê°œ)\")\n",
        "if use_class_weight:\n",
        "    print(f\"âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: ê°€ì¤‘ì¹˜ ì ìš© (ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì²˜ë¦¬)\")\n",
        "else:\n",
        "    print(f\"âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: ë°ì´í„° ì¦ê°• ì™„ë£Œ ({len(y_train_balanced)}ê°œ)\")\n",
        "print(f\"âœ… í”¼ì²˜ ì²˜ë¦¬: {X_train_processed.shape[1]}ê°œ í”¼ì²˜ (íŒŒìƒ í”¼ì²˜ í¬í•¨)\")\n",
        "if best_model:\n",
        "    print(f\"âœ… ëª¨ë¸ ì„ íƒ: {best_name} (ê²€ì¦ ì •í™•ë„ {best_score:.3f})\")\n",
        "    print(f\"âœ… ìµœì¢… ì˜ˆì¸¡: {len(final_predictions)}ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì˜ˆì¸¡ ì™„ë£Œ\")\n",
        "\n",
        "print(\"\\ní´ë˜ìŠ¤ë³„ ì²˜ë¦¬ ê²°ê³¼:\")\n",
        "final_class_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
        "for cls, count in final_class_counts.items():\n",
        "    original_count = pd.Series(y_train).value_counts().get(cls, 0)\n",
        "    print(f\"  {cls}: {original_count} â†’ {count} ({'ì¦ê°•' if count > original_count else 'ìœ ì§€'})\")\n",
        "\n",
        "print(\"\\nê¶Œì¥ì‚¬í•­:\")\n",
        "if use_class_weight:\n",
        "    print(\"1. ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ì „ì²´ í´ë˜ìŠ¤ ì˜ˆì¸¡ ì„±ëŠ¥ í™•ì¸ í•„ìš”\")\n",
        "    print(\"2. ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê·¼ë³¸ í•´ê²° ê¶Œì¥\")\n",
        "else:\n",
        "    print(\"1. ë°ì´í„° ì¦ê°•ì´ ì™„ë£Œë˜ì–´ ê· í˜•ì¡íŒ í•™ìŠµ ê°€ëŠ¥\")\n",
        "print(\"3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥\")\n",
        "print(\"4. ì•™ìƒë¸” ëª¨ë¸ ì ìš© ê³ ë ¤\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6s4YEfBq21s",
        "outputId": "61408d8a-bd15-4e12-be51-653670bf3eaf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ† ìµœì  ëª¨ë¸: RandomForest (ê²€ì¦ ì •í™•ë„: 0.870)\n",
            "\n",
            "=== ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.00      0.00      0.00         6\n",
            "           B       0.00      0.00      0.00         1\n",
            "           C       0.65      0.60      0.62       750\n",
            "           D       0.58      0.66      0.62      2054\n",
            "           E       0.95      0.93      0.94     11301\n",
            "\n",
            "    accuracy                           0.87     14112\n",
            "   macro avg       0.44      0.44      0.44     14112\n",
            "weighted avg       0.88      0.87      0.87     14112\n",
            "\n",
            "\n",
            "ë“±ê¸‰ë³„ ì˜ˆì¸¡ ì •í™•ë„:\n",
            "Aë“±ê¸‰: 0.000 (6ê°œ ìƒ˜í”Œ)\n",
            "Bë“±ê¸‰: 0.000 (1ê°œ ìƒ˜í”Œ)\n",
            "Cë“±ê¸‰: 0.596 (750ê°œ ìƒ˜í”Œ)\n",
            "Dë“±ê¸‰: 0.661 (2054ê°œ ìƒ˜í”Œ)\n",
            "Eë“±ê¸‰: 0.927 (11301ê°œ ìƒ˜í”Œ)\n",
            "\n",
            "=== ìƒìœ„ 20ê°œ ì¤‘ìš” í”¼ì²˜ ===\n",
            " 1. ì •ìƒì²­êµ¬ì›ê¸ˆ_B0M                          0.0383\n",
            " 2. ì²­êµ¬ê¸ˆì•¡_R6M                            0.0328\n",
            " 3. ì •ìƒì²­êµ¬ì›ê¸ˆ_B5M                          0.0279\n",
            " 4. ì²­êµ¬ê¸ˆì•¡_R3M                            0.0267\n",
            " 5. ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡                            0.0221\n",
            " 6. ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B2M                        0.0204\n",
            " 7. ì •ìƒì²­êµ¬ì›ê¸ˆ_B2M                          0.0200\n",
            " 8. ì¹´ë“œì´ìš©í•œë„ê¸ˆì•¡_B1M                        0.0182\n",
            " 9. ì´ìš©ê¸ˆì•¡_ì˜¤í”„ë¼ì¸_B0M                       0.0165\n",
            "10. ì •ìƒì…ê¸ˆì›ê¸ˆ_B2M                          0.0141\n",
            "11. í‰ì”_6M                               0.0140\n",
            "12. ì²­êµ¬ê¸ˆì•¡_B0                             0.0140\n",
            "13. ì •ìƒì…ê¸ˆì›ê¸ˆ_B5M                          0.0125\n",
            "14. ì´ìš©ê¸ˆì•¡ëŒ€                               0.0120\n",
            "15. CAí•œë„ê¸ˆì•¡                              0.0120\n",
            "16. ì •ìƒì…ê¸ˆì›ê¸ˆ_B0M                          0.0118\n",
            "17. ì´ìš©ê¸ˆì•¡_R3M_ì‹ ìš©ì²´í¬                       0.0113\n",
            "18. ì´ìš©ê¸ˆì•¡_ì¼ì‹œë¶ˆ_R12M                       0.0111\n",
            "19. _1ìˆœìœ„ì¹´ë“œì´ìš©ê¸ˆì•¡                          0.0107\n",
            "20. ì…íšŒì¼ì_ì‹ ìš©                             0.0105\n",
            "\n",
            "=== ìµœì¢… ì˜ˆì¸¡ ì¤€ë¹„ ===\n",
            "ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: (1440, 736)\n",
            "ìµœì¢… ì˜ˆì¸¡ ë“±ê¸‰ ë¶„í¬:\n",
            "C      73\n",
            "D     227\n",
            "E    1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ:\n",
            "             ID Predicted_Segment  Prob_A  Prob_C  Prob_D  Prob_E\n",
            "0  TRAIN_291453                 D     0.0    0.12    0.57    0.31\n",
            "1  TRAIN_104115                 E     0.0    0.29    0.29    0.42\n",
            "2  TRAIN_129210                 E     0.0    0.05    0.27    0.68\n",
            "3  TRAIN_194246                 E     0.0    0.01    0.05    0.94\n",
            "4  TRAIN_183742                 E     0.0    0.01    0.06    0.93\n",
            "\n",
            "==================================================\n",
            "=== í•™ìŠµ ê²€ì¦ ìš”ì•½ ===\n",
            "âœ… ë°ì´í„° ë¶„í• : Train 80% (56448ê°œ), Validation 20% (14112ê°œ)\n",
            "âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: ë°ì´í„° ì¦ê°• ì™„ë£Œ (180816ê°œ)\n",
            "âœ… í”¼ì²˜ ì²˜ë¦¬: 736ê°œ í”¼ì²˜ (íŒŒìƒ í”¼ì²˜ í¬í•¨)\n",
            "âœ… ëª¨ë¸ ì„ íƒ: RandomForest (ê²€ì¦ ì •í™•ë„ 0.870)\n",
            "âœ… ìµœì¢… ì˜ˆì¸¡: 1440ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì˜ˆì¸¡ ì™„ë£Œ\n",
            "\n",
            "í´ë˜ìŠ¤ë³„ ì²˜ë¦¬ ê²°ê³¼:\n",
            "  A: 22 â†’ 45204 (ì¦ê°•)\n",
            "  C: 3003 â†’ 45204 (ì¦ê°•)\n",
            "  D: 8216 â†’ 45204 (ì¦ê°•)\n",
            "  E: 45204 â†’ 45204 (ìœ ì§€)\n",
            "\n",
            "ê¶Œì¥ì‚¬í•­:\n",
            "1. ë°ì´í„° ì¦ê°•ì´ ì™„ë£Œë˜ì–´ ê· í˜•ì¡íŒ í•™ìŠµ ê°€ëŠ¥\n",
            "3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„  ê°€ëŠ¥\n",
            "4. ì•™ìƒë¸” ëª¨ë¸ ì ìš© ê³ ë ¤\n"
          ]
        }
      ]
    }
  ]
}